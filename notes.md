ud120-notes
==============


[Naive Bayes](https://github.com/FrancescoSTL/ud120-projects/tree/master/naive_bayes)
------
- Utilize *prior* and *posterior* probabilities in order to calculate the probability of any given feature appearing for each label. It is considered "naive" because it only considers the frequency at which features arise and does not look at their order.

Takeaways:
1. Easy to implement
2. Very good for large data-sets, especially text learning
3. Not good if you're interested in *phrases* rather than just words

[Support Vector Machines](https://github.com/FrancescoSTL/ud120-projects/tree/master/svm)
------
- Support vector machines utilize the concept of *margin* to determine the decision boundaries for classifiers. In splitting distance (margin) between data points in the set, the SVMs can create precise classifications. They utilize these 3 main concepts:

Parameters:
1. Kernel - defines the type of classification (linear, rbf, etc). Transforms data in a way that can be classified from non-classifiable data.
2. gamma - determines how far we should consider values away from the decision surface. Low = far, high = close
3. C - the level to which we attempt to fit to every data point. Low = smooth line & not worried about outliers as much, high = worried about outliers and exact classification

Takeaways:
1. Not suited for text learning
2. Takes a relatively long time to run
3. Prone to overfitting (too detailed decision boundary), esp if given high C

[Decision Trees](https://github.com/FrancescoSTL/ud120-projects/tree/master/decision_tree)
------
- Uses similar "trick" to do linear decision making in non-linear surfaces. Allow you to ask feature boundary questions once at a time in order to segment our decision boundary:
![Decision tree screenshot](https://github.com/FrancescoSTL/ud120-projects/assets/d-tree.png)

- Entropy - Controls how the decision tree determines where to split through the *measure of impurity in data across multiple data-sets*. Sets with the most purity in data are selected.
-- Defined as E = -Î£(i)[p(i) log(2)[p(i)]]. In other words: the negation of the summation of p(i)*(log base 2 of p(i)) from 0 to i
- Information gain = entropy(parent) - [weighted average]*entropy(children)
-- DT algorithm *maximizes* information gain
-- ![Decision tree entropy calculation example picture](https://github.com/FrancescoSTL/ud120-projects/assets/d-tree-entropy-calc.png)

Parameters:
1. min_samples_split - defines the minimum number of nodes needed in a segment in order to stop splitting. Lower = higher level of fitting (small, jagged blocks), higher = lower level of fitting (bigger, smoother blocks)
![Decision tree min_samples_split](https://github.com/FrancescoSTL/ud120-projects/assets/d-tree-min-samples.png)